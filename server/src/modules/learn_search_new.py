import json
import os

import requests
from dotenv import load_dotenv

from src.utils.xfyun_chat import XFYunChat

# è®¾ç½®å…¨å±€ä»£ç†
# os.environ["http_proxy"] = "http://127.0.0.1:7890"
# os.environ["https_proxy"] = "http://127.0.0.1:7890"


def google_cse_search(query, max_results=10):
    load_dotenv()
    API_KEY = os.getenv("GOOGLE_API_KEY")
    CX = os.getenv("GOOGLE_CSE_ID")
    url = "https://www.googleapis.com/customsearch/v1"
    params = {
        "key": API_KEY,
        "cx": CX,
        "q": query,
        "num": max_results,
    }
    try:
        proxies = {
            "http": "http://127.0.0.1:7890",
            "https": "http://127.0.0.1:7890"
        }
        resp = requests.get(url, params=params,proxies=proxies)
        data = resp.json()
    except Exception as e:
        print(f"è¯·æ±‚å¼‚å¸¸: {e}")
        return []

    #     # æ‰“å°å®Œæ•´è¿”å›å†…å®¹ï¼Œæ–¹ä¾¿æ’æŸ¥
    # print("APIè¿”å›å†…å®¹ï¼š")
    # print(json.dumps(data, indent=2, ensure_ascii=False))

    # æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯å­—æ®µ
    if "error" in data:
        error = data["error"]
        print(f"APIè¿”å›é”™è¯¯: ä»£ç  {error.get('code')}ï¼Œæ¶ˆæ¯: {error.get('message')}")
        return []

    results = []
    if "items" in data:
        for item in data["items"]:
            results.append({
                "title": item.get("title"),
                "link": item.get("link"),
                "snippet": item.get("snippet"),
            })

    print(f"æœç´¢å…³é”®è¯ï¼š{query}ï¼Œæ‰¾åˆ° {len(results)} æ¡ç»“æœ")
    return results

def llm_recon(keyword, content_summary):
    # 1. è¯»å–.envä¸­çš„è®¯é£æ˜Ÿç«APIä¿¡æ¯
    load_dotenv()
    SPARK_API_KEY = os.getenv("XFYUN_API_KEY")
    SPARK_API_SECRET = os.getenv("XFYUN_API_SECRET")
    SPARK_APP_ID = os.getenv("XFYUN_APPID")

    # 2. åˆå§‹åŒ–è®¯é£æ˜Ÿç«èŠå¤©å®ä¾‹
    search_chat = XFYunChat(
        appid=SPARK_APP_ID,
        api_key=SPARK_API_KEY,
        api_secret=SPARK_API_SECRET,
        system_prompt="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å­¦ä¹ èµ„æºæ¨èåŠ©æ‰‹ï¼Œèƒ½å¤Ÿå¸®åŠ©ç”¨æˆ·æ‰¾åˆ°ä¼˜è´¨çš„å­¦ä¹ èµ„æ–™ã€‚"
    )

    analysis_prompt = f"""
    ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å­¦ä¹ èµ„æºæ¨èåŠ©æ‰‹ï¼Œå–„äºåˆ†ææœç´¢ç»“æœå¹¶å¯¹å­¦ä¹ èµ„æºè¿›è¡Œåˆ†ç±»æ•´ç†ã€‚

    ğŸ¯ã€ä»»åŠ¡ç›®æ ‡ã€‘ï¼š
    è¯·æ ¹æ®ä»¥ä¸‹è¯¦ç»†çš„æœç´¢ç»“æœå†…å®¹ï¼Œå›´ç»•å…³é”®è¯ **"{keyword}"**ï¼Œå¯¹ä¸‹æ–¹æœç´¢ç»“æœè¿›è¡Œåˆ†ç±»æ•´ç†ï¼Œå¹¶åœ¨æŸäº›åˆ†ç±»èµ„æºç¨€ç¼ºæ—¶é€‚å½“è¡¥å……ä½ å·²çŸ¥çš„ä¼˜è´¨èµ„æ–™ã€‚

    ğŸ”ã€æœç´¢ç»“æœè¯´æ˜ã€‘ï¼š
    è¿™æ˜¯ä¸€ä¸ª JSON æ•°ç»„ï¼Œæ¯æ¡æ ¼å¼å¦‚ä¸‹ï¼š
    {{
      "title": "ç½‘é¡µæ ‡é¢˜",
      "link": "ç½‘é¡µé“¾æ¥",
      "snippet": "ç½‘é¡µç®€è¦æè¿°"
    }}

    ã€æœç´¢ç»“æœå†…å®¹ã€‘ï¼š
    {content_summary}
    
    ğŸ“šã€æ¨èç­–ç•¥ã€‘ï¼š
    1. ä¼˜å…ˆæ¨èä¸Šé¢æœç´¢ç»“æœä¸­çš„ä¼˜è´¨èµ„æº
    2. ä½ ä¹Ÿå¯ä»¥æ¨èå…¶ä»–ä½ çŸ¥é“çš„ä¼˜è´¨å­¦ä¹ èµ„æºï¼Œä½†è¦ç¡®ä¿é“¾æ¥çœŸå®æœ‰æ•ˆ
    3. æ¨èçš„èµ„æºåº”è¯¥æ¶µç›–ä¸åŒçš„å­¦ä¹ å¹³å°å’Œå½¢å¼

    ğŸ“šã€æ¨èåˆ†ç±»ã€‘ï¼ˆæŒ‰éœ€å½’ç±»ï¼‰ï¼š
    1. è§†é¢‘è¯¾ç¨‹ï¼ˆå¦‚ Bç«™ã€YouTubeã€è…¾è®¯è¯¾å ‚ã€Coursera ç­‰ï¼‰
    2. å­¦ä¹ æ–‡æ¡£ï¼ˆå¦‚å®˜æ–¹æ–‡æ¡£ã€æŠ€æœ¯åšå®¢ã€æ•™ç¨‹ç«™ç‚¹ç­‰ï¼‰
    3. å®æˆ˜é¡¹ç›® / å¼€æºä»£ç ï¼ˆå¦‚ GitHub é¡¹ç›®ï¼‰
    4. å…¶ä»–å­¦ä¹ å¹³å°æˆ–ç¤¾åŒºï¼ˆå¦‚ CSDNã€çŸ¥ä¹ã€æ˜é‡‘ã€LeetCodeï¼‰

    ğŸ“Œã€ä»»åŠ¡è¦æ±‚ã€‘ï¼š
    1. **ä¿ç•™æ‰€æœ‰æœç´¢ç»“æœä¸­çš„å†…å®¹**ï¼Œä¸åˆ é™¤ä¸é—æ¼ï¼Œé€æ¡åˆ†ææ¯ä¸ªæœç´¢ç»“æœçš„å…·ä½“å†…å®¹ï¼ŒåŒ…æ‹¬æ ‡é¢˜ã€æè¿°å¹¶è¿›è¡Œå½’ç±»ã€‚
    2. æ¯æ¡èµ„æºæŒ‰å¦‚ä¸‹æ ¼å¼æ•´ç†ï¼š
       - `name`: ç®€æ´æ ‡é¢˜ï¼ˆå–è‡ª `title`ï¼‰
       - `desc`: æ ¹æ® `snippet` ç²¾ç‚¼å†…å®¹ã€è¯´æ˜ä»·å€¼
       - `url`: å¯¹åº” `link`
    3. ä¸€æ¡å†…å®¹å¯å½’å…¥å¤šä¸ªåˆ†ç±»ï¼ˆå¦‚æ˜¯è§†é¢‘è¯¾ç¨‹ä¹Ÿæœ‰ä»£ç ï¼‰ï¼Œå¯é‡å¤å½’ç±»ã€‚
    4. **æ¯ä¸ªåˆ†ç±»å¿…é¡»è‡³å°‘æœ‰ 2 æ¡èµ„æº**ï¼Œå¦‚æœç´¢ç»“æœä¸è¶³ï¼Œè¯·ä½ é€‚å½“è¡¥å……é«˜è´¨é‡èµ„æ–™ã€‚
    5. æ¯ä¸ªåˆ†ç±»ä¸‹æ¨è 3-6 ä¸ªèµ„æºï¼Œæ ¼å¼æ¸…æ™°ï¼Œé¿å…é‡å¤ï¼Œå¯¹æ¯ä¸ªèµ„æºçš„æè¿°è¦å…·ä½“ä¸”æœ‰ä»·å€¼ã€‚. 
    5. é¿å…é‡å¤ï¼ˆå¦‚é“¾æ¥å®Œå…¨ä¸€è‡´æ—¶åªå‡ºç°ä¸€æ¬¡ï¼Œä½†å¯å¤šç±»å½’å±ï¼‰ã€‚

    ğŸ“¦ã€è¾“å‡ºæ ¼å¼ã€‘ï¼ˆæ ‡å‡† JSONï¼‰ï¼š
    {{
       "video_courses": [
        {{
          "name": "èµ„æºåç§°",
          "desc": "å†…å®¹ç‰¹ç‚¹å’Œé€‚ç”¨äººç¾¤",
          "url": "çœŸå®æœ‰æ•ˆé“¾æ¥",
        }},
        ...
      ],
      "learning_docs": [...],
      "projects": [...],
      "other_platforms": [...]
    }}

    âš ï¸ æ³¨æ„äº‹é¡¹ï¼š
    - ä¸åˆ é™¤æœç´¢ç»“æœä¸­çš„ä»»ä½•èµ„æºï¼›
    - æ‰€æœ‰åˆ†ç±»å­—æ®µä¸å¯ä¸ºç©ºï¼Œæ¯ç±»è‡³å°‘2æ¡ï¼›
    - JSON å¯è§£æï¼Œå­—æ®µåä¸€è‡´ï¼›
    - URL å¿…é¡»çœŸå®æœ‰æ•ˆï¼Œè‹¥é“¾æ¥å·²å¤±æ•ˆè¯·æ³¨æ˜ï¼›
    - åˆ†ç±»è¦ç²¾å‡†ï¼Œä¸æ··æ·†ç”¨é€”ï¼›

    è¯·å¯¹ä¸‹æ–¹æœç´¢ç»“æœå¼€å§‹åˆ†ç±»æ•´ç†å¹¶è¡¥å……èµ„æ–™ï¼ˆå¦‚æœ‰éœ€è¦ï¼‰ï¼š
    """

    recommendations = search_chat.chat(analysis_prompt)

    return recommendations

from concurrent.futures import ThreadPoolExecutor, as_completed
import requests
import json



def check_url(item):
    """æ£€æŸ¥å•ä¸ªURLæ˜¯å¦æœ‰æ•ˆï¼Œè¿”å› (æ˜¯å¦ä¿ç•™, item)"""
    try:
        PROXIES = {
            "http": "http://127.0.0.1:7890",
            "https": "http://127.0.0.1:7890"
        }
        response = requests.head(item["url"], timeout=3, proxies=PROXIES)
        if response.status_code == 200:
            return True, item
        else:
            print(f"âš ï¸ åˆ é™¤æ— æ•ˆé“¾æ¥: {item['url']} (çŠ¶æ€ç : {response.status_code})")
            return False, item
    except Exception as e:
        print(f"âš ï¸ åˆ é™¤æ— æ•ˆé“¾æ¥: {item['url']} (å¼‚å¸¸: {str(e)})")
        return False, item

def move_error_llm(recommendations, max_threads=4):
    try:
        if isinstance(recommendations, str):
            json_start = recommendations.find('{')
            json_end = recommendations.rfind('}') + 1
            if json_start != -1 and json_end != 0:
                json_str = recommendations[json_start:json_end]
                parsed_result = json.loads(json_str)

                cleaned_result = {}
                total_removed = 0

                for category_name, category_items in parsed_result.items():
                    if isinstance(category_items, list):
                        cleaned_items = []
                        # å¤šçº¿ç¨‹æ£€æŸ¥ URL
                        with ThreadPoolExecutor(max_workers=max_threads) as executor:
                            futures = {
                                executor.submit(check_url, item): item
                                for item in category_items if isinstance(item, dict) and 'url' in item
                            }

                            for future in as_completed(futures):
                                keep, item = future.result()
                                if keep:
                                    cleaned_items.append(item)
                                else:
                                    total_removed += 1

                        cleaned_result[category_name] = cleaned_items
                    else:
                        cleaned_result[category_name] = category_items

                if total_removed > 0:
                    print(f"å…±åˆ é™¤äº† {total_removed} ä¸ªæ— æ•ˆé“¾æ¥")

                # è¿”å›æ¸…ç†åçš„ JSON å­—ç¬¦ä¸²
                return json.dumps(cleaned_result, ensure_ascii=False, indent=2)
            else:
                return recommendations
        else:
            return recommendations
    except json.JSONDecodeError:
        print("JSONè§£æå¤±è´¥ï¼Œè¿”å›åŸå§‹ç»“æœ")
        return recommendations

def learn_search(query):
    results = google_cse_search(query)
    for result in results:
        print(f"æ ‡é¢˜: {result['title']}\né“¾æ¥: {result['link']}\næè¿°: {result['snippet']}\n")
    recommendations_json = llm_recon(keyword=query, content_summary=json.dumps(results, ensure_ascii=False))
    new_recommendations_json = move_error_llm(recommendations_json, max_threads=4)
    print(new_recommendations_json)

    return new_recommendations_json

if __name__ == "__main__":
    query = input("è¯·è¾“å…¥æœç´¢å…³é”®è¯ï¼š")
    learn_search(query)