import os
import json
from dotenv import load_dotenv
from concurrent.futures import ThreadPoolExecutor, as_completed

# æ›¿æ¢ä¸ºä½ çš„æ¨¡å‹æ–‡ä»¶è·¯å¾„
from src.utils.xfyun_chat import XFYunChat

load_dotenv(os.path.join(os.path.dirname(os.getcwd()), ".env"))
load_dotenv()

# åˆå§‹åŒ–è®¯é£å¤§æ¨¡å‹
kg_model = XFYunChat(
    appid=os.getenv("XFYUN_APPID"),
    api_key=os.getenv("XFYUN_API_KEY"),
    api_secret=os.getenv("XFYUN_API_SECRET"),
    system_prompt="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„çŸ¥è¯†å›¾è°±ä¸“å®¶ï¼Œèƒ½å¤Ÿå›´ç»•ä»»ä½•çŸ¥è¯†ç‚¹ç”Ÿæˆç»“æ„åŒ–çš„ä¸‰å…ƒç»„ã€‚"
)

summary_model = XFYunChat(
    appid=os.getenv("XFYUN_APPID"),
    api_key=os.getenv("XFYUN_API_KEY"),
    api_secret=os.getenv("XFYUN_API_SECRET"),
    system_prompt="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ€»ç»“ä¸“å®¶ï¼Œèƒ½å¤Ÿå¯¹ä¸€æ®µæ–‡æœ¬è¿›è¡Œæ€»ç»“,æå–ä¸€ä¸ªå…³é”®è¯ã€‚"
)


def summary_word(scene: str) -> list:
    """
    ä½¿ç”¨ LLM æå–é¢è¯•åé¦ˆä¸­çš„æŠ€æœ¯å…³é”®è¯ï¼Œå»æ‰æ ‡é¢˜ç­‰æ— å…³è¡Œã€‚
    """
    prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æŠ€æœ¯å›¾è°±ä¸“å®¶ï¼Œæ“…é•¿ä»æŠ€æœ¯é¢è¯•åé¦ˆä¸­æå–å€™é€‰äººçŸ¥è¯†è–„å¼±ç‚¹ã€‚

è¯·ä»ä¸‹é¢è¿™æ®µé¢è¯•è¯„è¯­ä¸­æå–å‡ºéœ€è¦è¡¥å¼ºçš„æŠ€æœ¯å…³é”®è¯ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºç®—æ³•åç§°ã€æ¨¡å—åç§°ã€æŠ€æœ¯æœ¯è¯­ã€çŸ¥è¯†ç‚¹ç­‰ï¼š
- æ¯è¡Œåªè¾“å‡ºä¸€ä¸ªå…³é”®è¯
- ä¸è¦åŠ æ ‡é¢˜ã€ä¸è¦åŠ è§£é‡Š

å†…å®¹å¦‚ä¸‹ï¼š
{scene}
"""
    response = summary_model.chat(prompt)
    lines = [line.strip() for line in response.strip().split("\n") if line.strip()]

    # è¿‡æ»¤æ‰æ˜æ˜¾ä¸æ˜¯å…³é”®è¯çš„è¡Œï¼ˆå¦‚æ ‡é¢˜ï¼‰
    keywords = [
        line for line in lines
        if not any(prefix in line for prefix in ["å…³é”®è¯", "è¡¥å¼º", "å…³é”®è¯å¦‚ä¸‹", "å¦‚ä¸‹"]) and len(line) < 30
    ]

    print(f"ğŸ“Œ æå–å…³é”®è¯ï¼š{keywords}")
    return keywords


# å•æ¬¡ç”Ÿæˆå‡½æ•°ï¼šå›´ç»•æŸä¸ªå®ä½“ç”Ÿæˆä¸‰å…ƒç»„
def generate_knowledge_triples(entity: str):
    prompt = f"""
è¯·å›´ç»•â€œ{entity}â€è¿™ä¸ªçŸ¥è¯†å…³é”®è¯ç”Ÿæˆç»“æ„åŒ–çŸ¥è¯†å›¾è°±ä¸‰å…ƒç»„ï¼ˆæ¯è¡Œä¸€ä¸ªJSONï¼‰ï¼š
{{"h": "{entity}", "r": "å…³ç³»", "t": "ç›¸å…³æ¦‚å¿µ"}}

è¦æ±‚ï¼š
1. è‡³å°‘8æ¡
2. â€œrâ€å¯ä»¥æ˜¯ï¼šå±äºã€åŒ…å«æ–¹æ³•ã€ä¼˜ç‚¹ã€ç¼ºç‚¹ã€åº”ç”¨é¢†åŸŸã€æ¶‰åŠé—®é¢˜ç­‰
3. ä»…è¾“å‡ºæ ‡å‡† JSON æ ¼å¼ï¼Œä¸è¦è§£é‡Šè¯´æ˜
"""
    response = kg_model.chat(prompt)
    triples = []
    for line in response.strip().split('\n'):
        try:
            triples.append(json.loads(line.strip()))
        except:
            continue
    return triples

# ğŸŒ± ä¸»æ„å›¾é€»è¾‘ï¼šé€’å½’æ‰©å±•çŸ¥è¯†å›¾è°±
# åœ¨é€’å½’ä¸­ï¼Œåªæ‰©å±•è¿™äº›å…³ç³»
ALLOWED_RELATIONS = {"å±äº", "æ˜¯", "æ˜¯ä¸€ç§", "å±äºç±»åˆ«", "å¯¹æ¯”æ–¹æ³•", "åŒ…å«æ–¹æ³•", "éš¶å±äº", "ç±»å‹"}

def build_knowledge_graph(seed_entity: str, max_depth: int = 2, max_nodes: int = 50):
    visited = set()
    queue = [(seed_entity, 0)]
    all_triples = []
    key = []

    while queue and len(visited) < max_nodes:
        current_entity, depth = queue.pop(0)
        if current_entity in visited or depth > max_depth:
            continue

        print(f"ğŸŒ± æ­£åœ¨æ‰©å±•ï¼š{current_entity} (æ·±åº¦ï¼š{depth})")
        if depth <= 1:
            key.append(current_entity)
        triples = generate_knowledge_triples(current_entity)
        visited.add(current_entity)

        for triple in triples:
            all_triples.append(triple)
            next_entity = triple.get("t")
            relation = triple.get("r")

            # âœ… åªæ‰©å±•çŸ¥è¯†æ¦‚å¿µç±»çš„â€œtâ€
            if relation in ALLOWED_RELATIONS and next_entity and next_entity not in visited:
                queue.append((next_entity, depth + 1))

    return all_triples, key

def build_knowledge_graph_from_keywords(keywords: list, output_dir: str = "kg_output", max_workers: int = 2):
    os.makedirs(output_dir, exist_ok=True)
    seen_triples = set()
    all_triples = []

    def process_keyword(seed):
        print(f"ğŸŒ± å¼€å§‹æ‰©å±•çŸ¥è¯†å›¾è°±ï¼š{seed}")
        triples = generate_knowledge_triples(seed)
        return seed, triples

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = [executor.submit(process_keyword, kw) for kw in keywords]
        for future in as_completed(futures):
            seed, graph = future.result()
            for triple in graph:
                triple_str = json.dumps(triple, ensure_ascii=False)
                if triple_str not in seen_triples:
                    seen_triples.add(triple_str)
                    all_triples.append(triple)

    # å†™å…¥åˆ°ç»Ÿä¸€æ–‡ä»¶
    # output_path = os.path.join(output_dir, "kg_expanded.jsonl")
    # with open(output_path, "w", encoding="utf-8") as f:
    #     for triple in all_triples:
    #         f.write(json.dumps(triple, ensure_ascii=False) + "\n")
    #
    # print(f"\nâœ… æ‰€æœ‰å›¾è°±æ‰©å±•å®Œæˆï¼Œåˆè®¡ç”Ÿæˆ {len(all_triples)} æ¡ä¸‰å…ƒç»„")
    # print(f"ğŸ“ è¾“å‡ºå·²ç»Ÿä¸€ä¿å­˜è‡³ï¼š{output_path}")
    return all_triples

# è¿è¡Œå…¥å£
if __name__ == "__main__":
    sentence = "ã€å‡†ç¡®ç‡ã€‘ï¼š60%\n\nã€åé¦ˆã€‘ï¼šå€™é€‰äººå›ç­”äº†è£…ç”²æ¿è¯†åˆ«æ¨¡å—å’Œæ¨¡å‹èåˆç­–ç•¥çš„éƒ¨åˆ†å†…å®¹ï¼Œä½†ç¼ºä¹ç»†èŠ‚å’Œå‡†ç¡®æ€§ã€‚è£…ç”²æ¿è¯†åˆ«æ¨¡å—çš„æè¿°è¿‡äºç®€ç•¥ï¼ŒæœªæåŠå…·ä½“å®ç°æ–¹æ³•ï¼›äººå·¥æ™ºèƒ½ç®—æ³•ç²¾è‹±èµ›éƒ¨åˆ†ä»…æåˆ°èåˆç­–ç•¥ï¼Œæœªæ¶‰åŠDEGCNã€SkateFormerç­‰æ¨¡å‹çš„é›†æˆè¿‡ç¨‹åŠæé«˜è¯†åˆ«ç²¾åº¦çš„å…·ä½“æªæ–½ã€‚\n\nã€æ”¹è¿›å»ºè®®ã€‘ï¼š\nâ€“ è¡¥å……è£…ç”²æ¿è¯†åˆ«æ¨¡å—çš„å…·ä½“å®ç°æ­¥éª¤ï¼Œå¦‚å›¾åƒé¢„å¤„ç†ã€ç‰¹å¾æå–ã€ç›®æ ‡æ£€æµ‹æµç¨‹ç­‰\nâ€“ è¯´æ˜åœ¨é›†æˆDEGCNã€SkateFormeræ—¶é‡‡ç”¨çš„æŠ€æœ¯ç»†èŠ‚ï¼ˆå¦‚æ¨¡å‹ç»“æ„è°ƒæ•´ã€è®­ç»ƒç­–ç•¥ç­‰ï¼‰\nâ€“ ä¸¾ä¾‹è¯´æ˜å¦‚ä½•é€šè¿‡å¤šæ¨¡å‹èåˆæå‡è¯†åˆ«ç²¾åº¦ï¼ˆå¦‚å¯¹æ¯”å•ä¸€æ¨¡å‹ä¸èåˆåçš„æ€§èƒ½æŒ‡æ ‡ï¼‰\nâ€“ æ˜ç¡®\"Soft Voting\"ç­–ç•¥ä¸­æƒé‡åˆ†é…çš„å…·ä½“ä¾æ®ï¼ˆå¦‚éªŒè¯é›†æ€§èƒ½æŒ‡æ ‡é‡åŒ–æ ‡å‡†ï¼‰"
    keywords = summary_word(sentence)
    print(keywords)
    output_dir = r"D:\AllFiles\competition\soft\Screening-LLM\data\graph"
    all_triplets = build_knowledge_graph_from_keywords(keywords)
    print(all_triplets)

    # seed = input("è¯·è¾“å…¥çŸ¥è¯†èµ·å§‹è¯ï¼š").strip()
    # graph = build_knowledge_graph(seed, max_depth=2, max_nodes=50)
    #
    # # ä¿å­˜
    # output_path = f"kg_{seed}_expanded.jsonl"
    # with open(output_path, "w", encoding="utf-8") as f:
    #     for triple in graph:
    #         f.write(json.dumps(triple, ensure_ascii=False) + "\n")
    #
    # print(f"\nâœ… çŸ¥è¯†å›¾è°±æ‰©å±•å®Œæˆï¼Œå…±ç”Ÿæˆ {len(graph)} æ¡ä¸‰å…ƒç»„")
    # print(f"ğŸ“ è¾“å‡ºå·²ä¿å­˜åˆ°ï¼š{output_path}")
